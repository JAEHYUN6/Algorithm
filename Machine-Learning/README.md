# 머신러닝 (ML)

--- 

### 검정

### 추정
  * 점추정
   - 모집단의 특성을 단일하 값을 추정하는 방법
   - 모수는 고정된 값
  * 구간추정
   -  모수가 있을 것으로 예상되는 구간을 정해놓고, 그 구간에 실제로 모수가 존재할 확률
  * 신뢰구간 
   -  (1-a) = 신뢰도
   -  100(1-a)% = 신뢰구간

---

### 편미분
   - 원하는 변수에 대해서마 미분
   - 그 외의 모든 것 상수 취급
 
###  연쇄 법칙
   - 새로운 변수 u를 추가하여 미분

---

###  평균 제곱 오차(MSE)
   - 회귀 문제에서 대표적인 손실 함수
   - 오차의 제곱의 평균

### 최소 제곱법(LSM)
   - 최적의 파라미터를 구할 수 있느 방법
   - 데이터에 대하 오차를 최소화

### 경사 하강법(Gradient Descent)
   - 손실 함수의 값을 최소화시키는 방향으로 파라미터를 업데이트
   - 함수의 최솟값은 무조건 순간 변화율이 0
   - 손실 함수에 대하 미분값이 0이 되는 방향을 파라미터의 업데이트 방향을 결정
   - 슈도 코드
      1. 현재 파라미터에서 손실 함수에 대하 미분값을 구함
      2. 미분값의 반대 방향으로 파라미터값을 업데이트
      3. 미분값이 0이 될 때까지 1~2번을 epoch만큼 반복 (epoch : Hyperparameter)
   
### 시그모이드 함수
  - 이진 분류 문제를 위한 비선형 함수
  - 함수의 출력값이 항상 0이상 1이하이며, 중앙 출력값은 0.5
	
### 소프트맥스 함수
  - 다중 분류 문제를 위한 비선형 함수


### Feature (= attribute)
  - 데이터 x의 특징, 항목을 의미
  - N: 데이터 샘플 갯수, D: 피처의 갯수
  
### Parameter (= weight)
  - 주어진 데이터 말고 모델이 가지고 있느 학습 가능한 파라미터
  - w로 표현
  
### Hyperparameter
  - 모델 학습에 있어 인간이 정해야 하는 변수들
  - 학습률, 배치 크기 등등

### 지도학습 
  * 훈련 데이터가 '입력'과 '타깃'으로 구성
  * 분류(Classification), 회귀(Regression)

### 비지도학습
  * '타깃'이 없는 훈련 데이터
  * 군집화(Clustering), 차원축소(Dimensionality reduction)
  
### 강화학습
  * 특정 환경에 최적화된 행동을 수행하고 수행에 대한 '보상'과 '현재 상태'를 받고 '최대한 많은 보상'을 위해 강화하는 학습
  
### 경사하강법
  * 함수 값이 낮아지는 방향으로 독립 변수르 변형시켜가면서 최종적을 최소값을 갖도록 하는 독립변수르 찾는 방법
  
### 선형 회귀
  - 단순 선형 회귀
     - 피처의 종류가 한 개인 데이터에 대한 회귀 모델

  - 다중 선형 회귀
     - 피처의 종류가 여러 개인 데이터에 대한 회귀 모델

  - 다항 회귀
     - 독립 변수의 차수를 높인 회귀 모델
---
# 라이브러리

* [Pandas](https://github.com/JAEHYUNYUK/Machine-Learning/blob/main/Pandas/Pandas.md)

* [sklearn](https://github.com/JAEHYUNYUK/Machine-Learning/blob/main/sklearn/sklearn.md)
